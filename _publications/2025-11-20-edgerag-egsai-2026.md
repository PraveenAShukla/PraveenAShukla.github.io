---
title: "EdgeRAG: Packable, Offline, and Fair Retrieval-Augmented Micro-Models for Low-Connectivity Regions"
collection: manuscripts
permalink: /publication/2025-11-20-edgerag-egsai-2026
date: 2025-11-20
venue: "AAAI 2026 EGSAI Workshop (accepted)"
excerpt: "Portable, fairness-aware offline RAG using 50-100 MB knowledge packs for low-connectivity devices; compressed multilingual retriever with dual answering path and equitable coreset selection."
citation: "Praveen A. Shukla. EdgeRAG: Packable, Offline, and Fair Retrieval-Augmented Micro-Models for Low-Connectivity Regions. AAAI 2026 EGSAI Workshop (accepted), 2025."
---

**Status:** Accepted to AAAI 2026 EGSAI Workshop.  
**Keywords:** offline RAG, knowledge packs, fairness-aware coreset selection, multilingual NLP, low-power deployment, edge AI, SMS/USSD, Global South, model compression, data efficiency  
**TL;DR:** Portable, fairness-aware offline RAG via small knowledge packs (50-100 MB) for low-connectivity, low-power devices; supports SMS/USSD and preserves minority-dialect coverage while enabling grounded answers.

## Abstract
We propose EdgeRAG: packable, offline, fairness-aware retrieval-augmented micro-models for low-connectivity regions. EdgeRAG compiles curated domain knowledge into small “knowledge packs” (50-100 MB) that run on entry-level Android phones and can be distributed via SD card, Bluetooth, or intermittently over 2G/SMS/USSD. The system couples a compressed multilingual retriever (≤128-dim, int8 product-quantized embeddings with saliency-aware sentence pruning and phrase-key caches) with a dual answering path: a tiny on-device generator (distilled 0.5–1.5B, INT8/LoRA) when resources permit, and a retrieval-only templating fallback that assembles grounded, cite-back answers for strictly offline devices. To ensure equity under compression, we introduce EquiCoverage, a fairness-aware coresetting procedure that maximizes semantic coverage while enforcing parity terms over dialect, topic, and geography metadata, preserving minority content that is typically dropped by naive size minimization. We outline an evaluation plan across health FAQs, agriculture advisories, and civic services in Swahili, Hausa, and Bangla plus English, measuring task success under 0G/2G/3G, CPU-only latency/energy on low-end phones, and group-wise answerability and citation diversity. Ablations study pack size (25-150 MB), embedding dimension (64-256), and LoRA vs. retrieval-only paths. EdgeRAG reframes AI access from always-online API calls to a portable, auditable file; by making knowledge local and updatable, it reduces cost and increases cultural fit for Global South deployments. This extended abstract describes ongoing work; artifacts (code, pack builder, and benchmarks) will be released upon acceptance.
